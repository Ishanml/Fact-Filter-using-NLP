{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOR0AYzksmdnOb33YjtPTsi",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Ishanml/Fact-Filter-using-NLP/blob/main/FactFilter.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Fact Filter using ML and NLP**\n"
      ],
      "metadata": {
        "id": "NSxgwMvv0GBl"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "8SWoxF8gICtz",
        "outputId": "2bfb7b60-7225-4ed3-b295-668743c1ae70"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fake News Data:\n",
            "                                               title  \\\n",
            "0   Donald Trump Sends Out Embarrassing New Year’...   \n",
            "1   Drunk Bragging Trump Staffer Started Russian ...   \n",
            "2   Sheriff David Clarke Becomes An Internet Joke...   \n",
            "3   Trump Is So Obsessed He Even Has Obama’s Name...   \n",
            "4   Pope Francis Just Called Out Donald Trump Dur...   \n",
            "\n",
            "                                                text subject  \\\n",
            "0  Donald Trump just couldn t wish all Americans ...    News   \n",
            "1  House Intelligence Committee Chairman Devin Nu...    News   \n",
            "2  On Friday, it was revealed that former Milwauk...    News   \n",
            "3  On Christmas day, Donald Trump announced that ...    News   \n",
            "4  Pope Francis used his annual Christmas Day mes...    News   \n",
            "\n",
            "                date  \n",
            "0  December 31, 2017  \n",
            "1  December 31, 2017  \n",
            "2  December 30, 2017  \n",
            "3  December 29, 2017  \n",
            "4  December 25, 2017  \n",
            "\n",
            "True News Data:\n",
            "                                               title  \\\n",
            "0  As U.S. budget fight looms, Republicans flip t...   \n",
            "1  U.S. military to accept transgender recruits o...   \n",
            "2  Senior U.S. Republican senator: 'Let Mr. Muell...   \n",
            "3  FBI Russia probe helped by Australian diplomat...   \n",
            "4  Trump wants Postal Service to charge 'much mor...   \n",
            "\n",
            "                                                text       subject  \\\n",
            "0  WASHINGTON (Reuters) - The head of a conservat...  politicsNews   \n",
            "1  WASHINGTON (Reuters) - Transgender people will...  politicsNews   \n",
            "2  WASHINGTON (Reuters) - The special counsel inv...  politicsNews   \n",
            "3  WASHINGTON (Reuters) - Trump campaign adviser ...  politicsNews   \n",
            "4  SEATTLE/WASHINGTON (Reuters) - President Donal...  politicsNews   \n",
            "\n",
            "                 date  \n",
            "0  December 31, 2017   \n",
            "1  December 29, 2017   \n",
            "2  December 31, 2017   \n",
            "3  December 30, 2017   \n",
            "4  December 29, 2017   \n",
            "\n",
            "Fake News Data Shape: (23481, 4)\n",
            "True News Data Shape: (21417, 4)\n"
          ]
        }
      ],
      "source": [
        "# 1) Data Loading and Exploring\n",
        "import pandas as pd\n",
        "\n",
        "# Load the datasets\n",
        "fake_df = pd.read_csv('Fake.csv')\n",
        "true_df = pd.read_csv('True.csv')\n",
        "\n",
        "# Display the first few rows of each dataset\n",
        "print(\"Fake News Data:\")\n",
        "print(fake_df.head())\n",
        "print(\"\\nTrue News Data:\")\n",
        "print(true_df.head())\n",
        "\n",
        "# Display the shape of each dataset\n",
        "print(\"\\nFake News Data Shape:\", fake_df.shape)\n",
        "print(\"True News Data Shape:\", true_df.shape)"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install spacy\n",
        "!pip install nltk\n",
        "!pip install pandas\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBzXOpCgR2JY",
        "outputId": "3105f801-9503-4e33-87a9-8b2cb283e3c0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: spacy in /usr/local/lib/python3.10/dist-packages (3.7.6)\n",
            "Requirement already satisfied: spacy-legacy<3.1.0,>=3.0.11 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.12)\n",
            "Requirement already satisfied: spacy-loggers<2.0.0,>=1.0.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.5)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.0.10)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.8)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.0.9)\n",
            "Requirement already satisfied: thinc<8.3.0,>=8.2.2 in /usr/local/lib/python3.10/dist-packages (from spacy) (8.2.5)\n",
            "Requirement already satisfied: wasabi<1.2.0,>=0.9.1 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.1.3)\n",
            "Requirement already satisfied: srsly<3.0.0,>=2.4.3 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.4.8)\n",
            "Requirement already satisfied: catalogue<2.1.0,>=2.0.6 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.0.10)\n",
            "Requirement already satisfied: weasel<0.5.0,>=0.1.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.4.1)\n",
            "Requirement already satisfied: typer<1.0.0,>=0.3.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (0.12.5)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (4.66.5)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.32.3)\n",
            "Requirement already satisfied: pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4 in /usr/local/lib/python3.10/dist-packages (from spacy) (2.8.2)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.1.4)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.10/dist-packages (from spacy) (71.0.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (24.1)\n",
            "Requirement already satisfied: langcodes<4.0.0,>=3.2.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (3.4.0)\n",
            "Requirement already satisfied: numpy>=1.19.0 in /usr/local/lib/python3.10/dist-packages (from spacy) (1.26.4)\n",
            "Requirement already satisfied: language-data>=1.2 in /usr/local/lib/python3.10/dist-packages (from langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: annotated-types>=0.4.0 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.20.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (2.20.1)\n",
            "Requirement already satisfied: typing-extensions>=4.6.1 in /usr/local/lib/python3.10/dist-packages (from pydantic!=1.8,!=1.8.1,<3.0.0,>=1.7.4->spacy) (4.12.2)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (3.8)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3.0.0,>=2.13.0->spacy) (2024.7.4)\n",
            "Requirement already satisfied: blis<0.8.0,>=0.7.8 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.7.11)\n",
            "Requirement already satisfied: confection<1.0.0,>=0.0.1 in /usr/local/lib/python3.10/dist-packages (from thinc<8.3.0,>=8.2.2->spacy) (0.1.5)\n",
            "Requirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (8.1.7)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (1.5.4)\n",
            "Requirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.10/dist-packages (from typer<1.0.0,>=0.3.0->spacy) (13.8.0)\n",
            "Requirement already satisfied: cloudpathlib<1.0.0,>=0.7.0 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (0.18.1)\n",
            "Requirement already satisfied: smart-open<8.0.0,>=5.2.1 in /usr/local/lib/python3.10/dist-packages (from weasel<0.5.0,>=0.1.0->spacy) (7.0.4)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->spacy) (2.1.5)\n",
            "Requirement already satisfied: marisa-trie>=0.7.7 in /usr/local/lib/python3.10/dist-packages (from language-data>=1.2->langcodes<4.0.0,>=3.2.0->spacy) (1.2.0)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (2.16.1)\n",
            "Requirement already satisfied: wrapt in /usr/local/lib/python3.10/dist-packages (from smart-open<8.0.0,>=5.2.1->weasel<0.5.0,>=0.1.0->spacy) (1.16.0)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0.0,>=0.3.0->spacy) (0.1.2)\n",
            "Requirement already satisfied: nltk in /usr/local/lib/python3.10/dist-packages (3.8.1)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.10/dist-packages (from nltk) (8.1.7)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.10/dist-packages (from nltk) (1.4.2)\n",
            "Requirement already satisfied: regex>=2021.8.3 in /usr/local/lib/python3.10/dist-packages (from nltk) (2024.5.15)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.10/dist-packages (from nltk) (4.66.5)\n",
            "Requirement already satisfied: pandas in /usr/local/lib/python3.10/dist-packages (2.1.4)\n",
            "Requirement already satisfied: numpy<2,>=1.22.4 in /usr/local/lib/python3.10/dist-packages (from pandas) (1.26.4)\n",
            "Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas) (2024.1)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas) (1.16.0)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 2) Data Preprocessing\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import nltk\n",
        "import spacy\n",
        "from nltk.corpus import stopwords\n",
        "from nltk.tokenize import word_tokenize\n",
        "from nltk.stem import PorterStemmer\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from multiprocessing import Pool\n",
        "\n",
        "# Download necessary NLTK data\n",
        "nltk.download('punkt')\n",
        "nltk.download('stopwords')\n",
        "nltk.download('wordnet')\n",
        "\n",
        "# Initialize the stemmer and lemmatizer\n",
        "stemmer = PorterStemmer()\n",
        "lemmatizer = WordNetLemmatizer()\n",
        "\n",
        "# Load SpaCy model\n",
        "nlp = spacy.load(\"en_core_web_sm\")\n",
        "\n",
        "# Define preprocessing function\n",
        "def preprocess(text):\n",
        "    # Tokenization\n",
        "    tokens = word_tokenize(text)\n",
        "\n",
        "    # Stop word removal\n",
        "    tokens = [word for word in tokens if word.lower() not in stopwords.words('english')]\n",
        "\n",
        "    # Stemming and Lemmatization\n",
        "    tokens = [stemmer.stem(word) for word in tokens]\n",
        "    tokens = [lemmatizer.lemmatize(word) for word in tokens]\n",
        "\n",
        "    return ' '.join(tokens)\n",
        "\n",
        "# Apply preprocessing using multiprocessing\n",
        "def parallel_preprocess(text_series, num_partitions=10):\n",
        "    data_split = np.array_split(text_series, num_partitions)\n",
        "    pool = Pool(num_partitions)\n",
        "    processed_data = pd.concat(pool.map(preprocess_partition, data_split))\n",
        "    pool.close()\n",
        "    pool.join()\n",
        "    return processed_data\n",
        "\n",
        "def preprocess_partition(text_series):\n",
        "    return text_series.apply(preprocess)\n",
        "\n",
        "# Load the datasets\n",
        "fake_df = pd.read_csv('Fake.csv')\n",
        "true_df = pd.read_csv('True.csv')\n",
        "\n",
        "# Add a label column to each dataframe\n",
        "fake_df['label'] = 0\n",
        "true_df['label'] = 1\n",
        "\n",
        "# Combine the datasets\n",
        "data = pd.concat([fake_df, true_df], axis=0).reset_index(drop=True)\n",
        "\n",
        "# Shuffle the data\n",
        "data = data.sample(frac=1).reset_index(drop=True)\n",
        "\n",
        "# Drop duplicates and missing values\n",
        "data = data.drop_duplicates()\n",
        "data = data.dropna()\n",
        "\n",
        "# Apply parallel preprocessing to text data\n",
        "data['processed_text'] = parallel_preprocess(data['text'])\n",
        "\n",
        "# Save the preprocessed data to avoid reprocessing in the future\n",
        "data.to_csv('preprocessed_data.csv', index=False)\n",
        "\n",
        "print(\"Data preprocessing complete.\")\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "A3q99B6iSs-j",
        "outputId": "8addb297-5178-439b-d85e-20ba2ff89ca7"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Unzipping tokenizers/punkt.zip.\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Unzipping corpora/stopwords.zip.\n",
            "[nltk_data] Downloading package wordnet to /root/nltk_data...\n",
            "/usr/local/lib/python3.10/dist-packages/numpy/core/fromnumeric.py:59: FutureWarning: 'Series.swapaxes' is deprecated and will be removed in a future version. Please use 'Series.transpose' instead.\n",
            "  return bound(*args, **kwds)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Data preprocessing complete.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#3) Model Training\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.ensemble import RandomForestClassifier, GradientBoostingClassifier\n",
        "from sklearn.tree import DecisionTreeClassifier\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer # Import TfidfVectorizer for text vectorization\n",
        "\n",
        "# Extract features (X) and labels (y) from the preprocessed data\n",
        "X = data['processed_text']  # Assuming 'processed_text' column contains the features\n",
        "y = data['label']  # Assuming 'label' column contains the labels\n",
        "\n",
        "# Vectorize the text data using TF-IDF\n",
        "vectorizer = TfidfVectorizer()\n",
        "X = vectorizer.fit_transform(X) # Transform text data to numerical vectors\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Initialize models\n",
        "models = {\n",
        "    \"Logistic Regression\": LogisticRegression(),\n",
        "    \"Random Forest\": RandomForestClassifier(),\n",
        "    \"Decision Tree\": DecisionTreeClassifier(),\n",
        "    \"Gradient Boosting\": GradientBoostingClassifier()\n",
        "}\n",
        "\n",
        "# Train models\n",
        "for name, model in models.items():\n",
        "    print(f\"Training {name}...\")\n",
        "    model.fit(X_train, y_train)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2iAznqRyIXjX",
        "outputId": "68f6d7ec-9887-4335-cec2-205e92f44dbf"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Training Logistic Regression...\n",
            "Training Random Forest...\n",
            "Training Decision Tree...\n",
            "Training Gradient Boosting...\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# 4) Model Evaluation\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "\n",
        "# Evaluate models\n",
        "for name, model in models.items():\n",
        "    print(f\"\\n{name} Evaluation:\")\n",
        "    y_pred = model.predict(X_test)\n",
        "    print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "    print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "    print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "    print(\"F1 Score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "k-pwABF2IbTS",
        "outputId": "2e1f4648-faa9-4a9f-f762-2c688239f8a1"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Logistic Regression Evaluation:\n",
            "Accuracy: 0.9898187513985232\n",
            "Precision: 0.9880411384836164\n",
            "Recall: 0.9901725790987536\n",
            "F1 Score: 0.9891057105231653\n",
            "\n",
            "Random Forest Evaluation:\n",
            "Accuracy: 0.9946296710673529\n",
            "Precision: 0.9951969260326609\n",
            "Recall: 0.9932885906040269\n",
            "F1 Score: 0.9942418426103646\n",
            "\n",
            "Decision Tree Evaluation:\n",
            "Accuracy: 0.9963078988588051\n",
            "Precision: 0.9959261921878745\n",
            "Recall: 0.9961649089165868\n",
            "F1 Score: 0.9960455362492511\n",
            "\n",
            "Gradient Boosting Evaluation:\n",
            "Accuracy: 0.9966435444170956\n",
            "Precision: 0.9954545454545455\n",
            "Recall: 0.9973633748801534\n",
            "F1 Score: 0.9964080459770116\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Hyperparameter Tuning\n",
        "from sklearn.model_selection import GridSearchCV\n",
        "\n",
        "# Define parameter grid for Logistic Regression\n",
        "param_grid_lr = {\n",
        "    'C': [0.1, 1, 10],\n",
        "    'solver': ['liblinear']\n",
        "}\n",
        "\n",
        "# Initialize GridSearchCV\n",
        "grid_lr = GridSearchCV(LogisticRegression(), param_grid_lr, cv=5, scoring='accuracy')\n",
        "\n",
        "# Train and tune the model\n",
        "grid_lr.fit(X_train, y_train)\n",
        "\n",
        "# Get the best model\n",
        "best_lr = grid_lr.best_estimator_\n",
        "\n",
        "# Evaluate the best model\n",
        "y_pred = best_lr.predict(X_test)\n",
        "print(\"\\nTuned Logistic Regression Evaluation:\")\n",
        "print(\"Accuracy:\", accuracy_score(y_test, y_pred))\n",
        "print(\"Precision:\", precision_score(y_test, y_pred))\n",
        "print(\"Recall:\", recall_score(y_test, y_pred))\n",
        "print(\"F1 Score:\", f1_score(y_test, y_pred))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XXgJZOqLIfAn",
        "outputId": "95b06324-59b9-4059-b167-578a0285b509"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Tuned Logistic Regression Evaluation:\n",
            "Accuracy: 0.9942940255090624\n",
            "Precision: 0.9930605407992342\n",
            "Recall: 0.9947267497603068\n",
            "F1 Score: 0.9938929469524608\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "qkgUqL300AhN"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}